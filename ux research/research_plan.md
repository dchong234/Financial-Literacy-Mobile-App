# Phase 1 Research Plan: Financial Literacy App

**Timeline:** Days 1–4 (Dec 16 – Dec 19)
**Goal:** Validate assumptions about financial illiteracy and define the "AI Advisor" persona.

---

## Method 1: Competitor Analysis (The Landscape)

### Research Objective
To identify gaps in existing financial apps (e.g., Mint, YNAB, Robinhood, Acorns) specifically regarding accessibility for low-literacy users. We assume most apps rely on jargon or complex charts that alienate our target audience.

### Key Research Questions
1.  How do current apps handle onboarding for users with zero financial knowledge?
2.  What specific terminology (e.g., "ETF", "Yield", "Diversification") causes the most friction?
3.  How is "trust" established visually and verbally in successful fintech apps?
4.  What are the "anti-patterns" that make users feel stupid or overwhelmed?

### Target Participants
*   **N/A (Desk Research)**
*   **Analyst:** UX Researcher (You)

### Procedure & Timeline
*   **Day 1 (4 hours):**
    *   Download 3-5 top financial apps (Acorns, Stash, Mint/Credit Karma, classic banking app).
    *   Walk through the onboarding flow as a "novice" user.
    *   Screenshot every screen that contains jargon or complex data.
    *   Annotate screenshots with "Friction Points" and "Trust Signals".

### Data Collected
*   **Artifacts:** Screenshot audit deck with annotations.
*   **Data:** List of "Banned Words" (jargon) and "Friendly Alternatives".

### Expected Outputs & Deliverables
*   **"Jargon Blacklist":** A list of terms to avoid or strictly define in our app.
*   **Onboarding Best Practices:** A checklist of "Must-Haves" for our Day 1 design.
*   **Why it matters:** Prevents us from reinventing the wheel or repeating competitors' mistakes.

---

## Method 2: In-Depth Interviews (The User)

### Research Objective
To understand the emotional relationship users have with money and uncover the root causes of their "financial paralysis." We need to validate if "fear of judgment" is a bigger blocker than "lack of knowledge."

### Key Research Questions
1.  Can you walk me through the last time you tried to make a budget or investment decision? What happened?
2.  How do you currently track your money? (e.g., mental math, notebook, ignoring it?)
3.  What is your biggest fear when you hear the word "investing"?
4.  If you had a magic wand, what would make you feel "safe" managing your money?

### Target Participants
*   **Profile:** Adults (25-45) who self-identify as "bad with money" or "financially anxious." No prior investment experience.
*   **Count:** 5 Participants.
*   **Recruitment:** Friends of friends, local community boards (scrappy recruitment for speed).

### Procedure & Timeline
*   **Day 1:** Recruit and schedule.
*   **Day 2-3:** Conduct 5 x 45-minute remote interviews (Zoom/Google Meet).
    *   **Warm-up (5 min):** Build rapport, assure non-judgment.
    *   **Current State (15 min):** "Show me how you check your bank balance."
    *   **Pain Points (15 min):** Discuss past failures or anxieties.
    *   **Magic Wand (10 min):** Ideal solution brainstorming.

### Data Collected
*   **Qualitative:** Audio recordings, transcripts, emotional maps (e.g., "User felt shame when discussing debt").
*   **Artifacts:** "Financial Trauma" quotes.

### Expected Outputs & Deliverables
*   **User Personas:** 2 primary personas (e.g., "The Ostrich" who ignores money, "The Anxious Spender").
*   **Empathy Map:** Visualizing the user's feelings vs. actions.
*   **Why it matters:** Ensures we design for *emotion* (reducing anxiety), not just *utility* (tracking numbers).

---

## Method 3: Concept Testing - "The AI Trust Test" (The Solution)

### Research Objective
To test if financially illiterate users will trust an AI advisor and what "personality" that AI needs to have. We assume users might mistrust a "robot" with their money unless it feels human and empathetic.

### Key Research Questions
1.  Do users prefer an AI that sounds "Professional/Banker-like" or "Friendly/Coach-like"?
2.  What questions do they *actually* want to ask an AI advisor?
3.  At what point does the AI feel "creepy" or "unsafe"?
4.  Does "transparency" (explaining *why* a suggestion was made) increase trust?

### Target Participants
*   **Same 5 participants** from the Interviews (append 15 mins to the end of the interview or do a follow-up).

### Procedure & Timeline
*   **Day 2-3 (During Interviews):**
    *   **Wizard of Oz Test:** Show 2 distinct "Chat Concepts" (static images or text).
        *   *Concept A:* "Based on your $500 savings, I recommend VOO ETF." (Cold, Direct)
        *   *Concept B:* "Great job saving $500! Since you're just starting, a safe bet is a 'basket of stocks' called an ETF. It spreads your risk. Want to try?" (Warm, Educational)
    *   Ask: "Which one would you listen to? Why?"
    *   Ask: "Type one question you'd be too embarrassed to ask a human bank teller."

### Data Collected
*   **Qualitative:** Preference data (A vs B).
*   **Content:** Real questions users would ask an AI.

### Expected Outputs & Deliverables
*   **AI Persona Guide:** Voice & Tone guidelines for the AI (e.g., "The Supportive Big Sibling").
*   **Trust Framework:** UI elements required to make the AI feel safe (e.g., "Why I said this" button).
*   **Why it matters:** If the AI feels condescending or robotic, the core feature of the app fails.

---

## Summary of Deliverables (End of Day 4)
1.  **Jargon Blacklist & Onboarding Checklist** (from Competitor Analysis).
2.  **User Personas & Empathy Maps** (from Interviews).
3.  **AI Persona & Trust Guidelines** (from Concept Testing).
